{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import torch.utils.data\n",
    "# from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import argparse\n",
    "from udlp.autoencoder.denoisingAutoencoder import DenoisingAutoencoder\n",
    "from pacdataset import PacDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "lr = 0.001\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEA with PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"/Users/josephmann/Documents/Gheiratmand/sMRI competition/PAC Data/pac2018/\"\n",
    "datasets = {x: PacDataset(train=(x=='train'), \n",
    "                          root_dir = \"/home/paperspace/data/pac2018/\") \n",
    "#                           root_dir = \"/Users/josephmann/Documents/Gheiratmand/sMRI competition/PAC Data/pac2018/\") \n",
    "            for x in ['val','train']}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets['train'],\n",
    "    batch_size= batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets['val'],\n",
    "    batch_size= batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Denoising Autoencoding layer=======\n",
      "####Epoch 0: Valid Reconstruct Loss: 12172.864\n",
      "#Epoch   1: Reconstruct Loss: 3535.038, Valid Reconstruct Loss: 3430.565\n",
      "#Epoch   2: Reconstruct Loss: 3336.803, Valid Reconstruct Loss: 3370.363\n"
     ]
    }
   ],
   "source": [
    "in_features = 17545\n",
    "out_features = 500\n",
    "dae = DenoisingAutoencoder(in_features, out_features)\n",
    "dae.fit(train_loader, test_loader, lr=lr, num_epochs=epochs, loss_type=\"cross-entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDEA with PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from udlp.autoencoder.stackedDAE import StackedDAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l is  1\n",
      "=====Denoising Autoencoding layer=======\n",
      "####Epoch 0: Valid Reconstruct Loss: 12172.833\n",
      "#Epoch   1: Reconstruct Loss: 3448.230, Valid Reconstruct Loss: 3432.119\n",
      "#Epoch   2: Reconstruct Loss: 3290.231, Valid Reconstruct Loss: 3415.285\n",
      "#Epoch   3: Reconstruct Loss: 3252.071, Valid Reconstruct Loss: 3327.590\n",
      "#Epoch   4: Reconstruct Loss: 3225.990, Valid Reconstruct Loss: 3287.294\n",
      "#Epoch   5: Reconstruct Loss: 3185.519, Valid Reconstruct Loss: 3213.797\n",
      "#Epoch   6: Reconstruct Loss: 3164.896, Valid Reconstruct Loss: 3246.681\n",
      "#Epoch   7: Reconstruct Loss: 3145.402, Valid Reconstruct Loss: 3229.304\n",
      "#Epoch   8: Reconstruct Loss: 3130.313, Valid Reconstruct Loss: 3270.102\n",
      "#Epoch   9: Reconstruct Loss: 3119.263, Valid Reconstruct Loss: 3260.647\n",
      "#Epoch  10: Reconstruct Loss: 3111.605, Valid Reconstruct Loss: 3218.279\n",
      "#Epoch  11: Reconstruct Loss: 3102.711, Valid Reconstruct Loss: 3213.577\n",
      "#Epoch  12: Reconstruct Loss: 3093.227, Valid Reconstruct Loss: 3199.374\n",
      "#Epoch  13: Reconstruct Loss: 3094.683, Valid Reconstruct Loss: 3217.766\n",
      "#Epoch  14: Reconstruct Loss: 3082.722, Valid Reconstruct Loss: 3209.871\n",
      "#Epoch  15: Reconstruct Loss: 3078.067, Valid Reconstruct Loss: 3199.543\n",
      "#Epoch  16: Reconstruct Loss: 3074.625, Valid Reconstruct Loss: 3171.700\n",
      "#Epoch  17: Reconstruct Loss: 3071.011, Valid Reconstruct Loss: 3142.775\n",
      "#Epoch  18: Reconstruct Loss: 3069.463, Valid Reconstruct Loss: 3223.516\n",
      "#Epoch  19: Reconstruct Loss: 3064.236, Valid Reconstruct Loss: 3190.306\n",
      "#Epoch  20: Reconstruct Loss: 3066.562, Valid Reconstruct Loss: 3144.281\n",
      "type(data_x) <class 'torch.FloatTensor'>\n",
      "type(valid_x) <class 'torch.FloatTensor'>\n",
      "l is  2\n",
      "=====Denoising Autoencoding layer=======\n",
      "####Epoch 0: Valid Reconstruct Loss: 959.721\n",
      "#Epoch   1: Reconstruct Loss: 430.265, Valid Reconstruct Loss: 178.906\n",
      "#Epoch   2: Reconstruct Loss: 283.404, Valid Reconstruct Loss: 142.817\n",
      "#Epoch   3: Reconstruct Loss: 255.876, Valid Reconstruct Loss: 120.547\n",
      "#Epoch   4: Reconstruct Loss: 242.213, Valid Reconstruct Loss: 86.686\n",
      "#Epoch   5: Reconstruct Loss: 231.222, Valid Reconstruct Loss: 88.544\n",
      "#Epoch   6: Reconstruct Loss: 227.256, Valid Reconstruct Loss: 79.068\n",
      "#Epoch   7: Reconstruct Loss: 221.293, Valid Reconstruct Loss: 68.271\n",
      "#Epoch   8: Reconstruct Loss: 217.146, Valid Reconstruct Loss: 70.349\n",
      "#Epoch   9: Reconstruct Loss: 217.620, Valid Reconstruct Loss: 77.261\n",
      "#Epoch  10: Reconstruct Loss: 214.574, Valid Reconstruct Loss: 74.476\n",
      "#Epoch  11: Reconstruct Loss: 215.098, Valid Reconstruct Loss: 75.406\n",
      "#Epoch  12: Reconstruct Loss: 210.169, Valid Reconstruct Loss: 85.770\n",
      "#Epoch  13: Reconstruct Loss: 213.219, Valid Reconstruct Loss: 83.732\n",
      "#Epoch  14: Reconstruct Loss: 205.737, Valid Reconstruct Loss: 86.566\n",
      "#Epoch  15: Reconstruct Loss: 209.034, Valid Reconstruct Loss: 82.042\n",
      "#Epoch  16: Reconstruct Loss: 205.029, Valid Reconstruct Loss: 88.679\n",
      "#Epoch  17: Reconstruct Loss: 204.802, Valid Reconstruct Loss: 88.417\n",
      "#Epoch  18: Reconstruct Loss: 200.652, Valid Reconstruct Loss: 87.965\n",
      "#Epoch  19: Reconstruct Loss: 201.378, Valid Reconstruct Loss: 88.729\n",
      "#Epoch  20: Reconstruct Loss: 198.132, Valid Reconstruct Loss: 89.164\n",
      "type(data_x) <class 'torch.FloatTensor'>\n",
      "type(valid_x) <class 'torch.FloatTensor'>\n",
      "l is  3\n",
      "=====Denoising Autoencoding layer=======\n",
      "####Epoch 0: Valid Reconstruct Loss: 264.553\n",
      "#Epoch   1: Reconstruct Loss: 186.721, Valid Reconstruct Loss: 71.438\n",
      "#Epoch   2: Reconstruct Loss: 120.867, Valid Reconstruct Loss: 61.818\n",
      "#Epoch   3: Reconstruct Loss: 105.739, Valid Reconstruct Loss: 59.769\n",
      "#Epoch   4: Reconstruct Loss: 98.959, Valid Reconstruct Loss: 55.918\n",
      "#Epoch   5: Reconstruct Loss: 94.144, Valid Reconstruct Loss: 59.789\n",
      "#Epoch   6: Reconstruct Loss: 90.438, Valid Reconstruct Loss: 57.516\n",
      "#Epoch   7: Reconstruct Loss: 88.469, Valid Reconstruct Loss: 59.993\n",
      "#Epoch   8: Reconstruct Loss: 89.091, Valid Reconstruct Loss: 59.517\n",
      "#Epoch   9: Reconstruct Loss: 85.700, Valid Reconstruct Loss: 61.108\n",
      "#Epoch  10: Reconstruct Loss: 84.503, Valid Reconstruct Loss: 60.659\n",
      "#Epoch  11: Reconstruct Loss: 79.664, Valid Reconstruct Loss: 62.475\n",
      "#Epoch  12: Reconstruct Loss: 79.193, Valid Reconstruct Loss: 65.292\n",
      "#Epoch  13: Reconstruct Loss: 76.074, Valid Reconstruct Loss: 65.185\n",
      "#Epoch  14: Reconstruct Loss: 73.978, Valid Reconstruct Loss: 67.175\n",
      "#Epoch  15: Reconstruct Loss: 73.834, Valid Reconstruct Loss: 65.736\n",
      "#Epoch  16: Reconstruct Loss: 73.193, Valid Reconstruct Loss: 65.701\n",
      "#Epoch  17: Reconstruct Loss: 71.710, Valid Reconstruct Loss: 69.578\n",
      "#Epoch  18: Reconstruct Loss: 70.637, Valid Reconstruct Loss: 66.190\n",
      "#Epoch  19: Reconstruct Loss: 67.421, Valid Reconstruct Loss: 70.692\n",
      "#Epoch  20: Reconstruct Loss: 68.391, Valid Reconstruct Loss: 69.655\n",
      "type(data_x) <class 'torch.FloatTensor'>\n",
      "type(valid_x) <class 'torch.FloatTensor'>\n",
      "l is  4\n",
      "=====Denoising Autoencoding layer=======\n",
      "####Epoch 0: Valid Reconstruct Loss: 112.001\n",
      "#Epoch   1: Reconstruct Loss: 206.798, Valid Reconstruct Loss: 42.959\n",
      "#Epoch   2: Reconstruct Loss: 184.576, Valid Reconstruct Loss: 41.574\n",
      "#Epoch   3: Reconstruct Loss: 183.415, Valid Reconstruct Loss: 41.255\n",
      "#Epoch   4: Reconstruct Loss: 182.732, Valid Reconstruct Loss: 41.156\n",
      "#Epoch   5: Reconstruct Loss: 182.310, Valid Reconstruct Loss: 41.055\n",
      "#Epoch   6: Reconstruct Loss: 181.898, Valid Reconstruct Loss: 40.802\n",
      "#Epoch   7: Reconstruct Loss: 181.471, Valid Reconstruct Loss: 40.524\n",
      "#Epoch   8: Reconstruct Loss: 180.871, Valid Reconstruct Loss: 40.461\n",
      "#Epoch   9: Reconstruct Loss: 180.324, Valid Reconstruct Loss: 40.145\n",
      "#Epoch  10: Reconstruct Loss: 179.584, Valid Reconstruct Loss: 39.991\n",
      "#Epoch  11: Reconstruct Loss: 178.798, Valid Reconstruct Loss: 39.668\n",
      "#Epoch  12: Reconstruct Loss: 178.219, Valid Reconstruct Loss: 39.486\n",
      "#Epoch  13: Reconstruct Loss: 177.773, Valid Reconstruct Loss: 39.653\n",
      "#Epoch  14: Reconstruct Loss: 177.391, Valid Reconstruct Loss: 39.257\n",
      "#Epoch  15: Reconstruct Loss: 177.084, Valid Reconstruct Loss: 39.090\n",
      "#Epoch  16: Reconstruct Loss: 176.844, Valid Reconstruct Loss: 38.992\n",
      "#Epoch  17: Reconstruct Loss: 176.642, Valid Reconstruct Loss: 38.929\n",
      "#Epoch  18: Reconstruct Loss: 176.383, Valid Reconstruct Loss: 38.821\n",
      "#Epoch  19: Reconstruct Loss: 176.329, Valid Reconstruct Loss: 38.812\n",
      "#Epoch  20: Reconstruct Loss: 176.286, Valid Reconstruct Loss: 38.850\n",
      "type(data_x) <class 'torch.FloatTensor'>\n",
      "type(valid_x) <class 'torch.FloatTensor'>\n",
      "=====Stacked Denoising Autoencoding layer=======\n",
      "#Epoch 0: Valid Reconstruct Loss: 4235.129\n",
      "#Epoch   1: Reconstruct Loss: 3286.703, Valid Reconstruct Loss: 3269.334\n",
      "#Epoch   2: Reconstruct Loss: 3246.943, Valid Reconstruct Loss: 3306.027\n",
      "#Epoch   3: Reconstruct Loss: 3237.410, Valid Reconstruct Loss: 3313.807\n",
      "#Epoch   4: Reconstruct Loss: 3234.134, Valid Reconstruct Loss: 3218.281\n",
      "#Epoch   5: Reconstruct Loss: 3222.123, Valid Reconstruct Loss: 3240.171\n",
      "#Epoch   6: Reconstruct Loss: 3216.456, Valid Reconstruct Loss: 3235.506\n",
      "#Epoch   7: Reconstruct Loss: 3210.440, Valid Reconstruct Loss: 3212.967\n",
      "#Epoch   8: Reconstruct Loss: 3201.608, Valid Reconstruct Loss: 3226.110\n",
      "#Epoch   9: Reconstruct Loss: 3199.797, Valid Reconstruct Loss: 3613.269\n",
      "#Epoch  10: Reconstruct Loss: 3201.948, Valid Reconstruct Loss: 3253.025\n",
      "#Epoch  11: Reconstruct Loss: 3196.056, Valid Reconstruct Loss: 3242.280\n",
      "#Epoch  12: Reconstruct Loss: 3193.401, Valid Reconstruct Loss: 3265.867\n",
      "#Epoch  13: Reconstruct Loss: 3196.279, Valid Reconstruct Loss: 3243.510\n",
      "#Epoch  14: Reconstruct Loss: 3179.324, Valid Reconstruct Loss: 3280.790\n",
      "#Epoch  15: Reconstruct Loss: 3179.248, Valid Reconstruct Loss: 3231.205\n",
      "#Epoch  16: Reconstruct Loss: 3180.234, Valid Reconstruct Loss: 3251.786\n",
      "#Epoch  17: Reconstruct Loss: 3174.600, Valid Reconstruct Loss: 3227.802\n",
      "#Epoch  18: Reconstruct Loss: 3174.224, Valid Reconstruct Loss: 3203.093\n",
      "#Epoch  19: Reconstruct Loss: 3173.457, Valid Reconstruct Loss: 3249.588\n",
      "#Epoch  20: Reconstruct Loss: 3170.778, Valid Reconstruct Loss: 3294.097\n"
     ]
    }
   ],
   "source": [
    "# in_features = 784\n",
    "in_features = 17545\n",
    "out_features = 500\n",
    "pretrainepochs = 20 \n",
    "epochs = 20\n",
    "\n",
    "sdae = StackedDAE(input_dim=in_features, z_dim=10, binary=True,\n",
    "    encodeLayer=[500,500,2000], decodeLayer=[2000,500,500], activation=\"relu\", \n",
    "    dropout=0)\n",
    "sdae.pretrain(train_loader, test_loader, lr=lr, batch_size= batch_size, \n",
    "    num_epochs=pretrainepochs, corrupt=0.3, loss_type=\"cross-entropy\")\n",
    "sdae.save_model(\"model/sdae.pt\")\n",
    "sdae.fit(train_loader, test_loader, lr= lr, num_epochs= epochs, corrupt=0.3, loss_type=\"cross-entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdae.save_model(\"model/pre_vade.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test VaDE with PAC and pre-trained model\n",
    "just for file format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from model/pre_vade.pt...\n",
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 4140.25597\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3241.66117, Valid Loss: 3192.90484, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3184.89950, Valid Loss: 3206.42785, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3169.68459, Valid Loss: 3202.02557, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3170.44605, Valid Loss: 3185.52840, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3162.35217, Valid Loss: 3196.19272, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3171.72202, Valid Loss: 3184.59456, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3168.05140, Valid Loss: 3174.83904, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3158.79186, Valid Loss: 3212.82876, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3151.82832, Valid Loss: 3219.81215, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3155.57900, Valid Loss: 3219.58857, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3164.65678, Valid Loss: 3181.88866, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3145.01423, Valid Loss: 3190.64643, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3143.73049, Valid Loss: 3195.79738, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3160.15543, Valid Loss: 3191.97362, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3149.24420, Valid Loss: 3184.50306, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3136.62730, Valid Loss: 3189.83244, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3135.73869, Valid Loss: 3174.86144, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3137.64300, Valid Loss: 3180.55859, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3131.77725, Valid Loss: 3174.23543, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3127.84978, Valid Loss: 3177.00404, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "from udlp.clustering.vade import VaDE\n",
    "args_pretrain = 'model/pre_vade.pt'\n",
    "\n",
    "vade = VaDE(input_dim=in_features, z_dim=10, n_centroids=10, binary=True,\n",
    "        encodeLayer=[500,500,2000], decodeLayer=[2000,500,500])\n",
    "\n",
    "if args_pretrain != \"\":\n",
    "    print(\"Loading model from %s...\" % args_pretrain)\n",
    "    vade.load_model(args_pretrain)\n",
    "print(\"Initializing through GMM..\")\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.save_model(\"model/vade2k5c5c_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.load_model(\"model/vade1.pt\")\n",
    "print(\"Initializing through GMM..\")\n",
    "batch_size = 32\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 429522.77846\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 4815.66560, Valid Loss: 3318.81995, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3321.31552, Valid Loss: 3295.89734, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3319.53538, Valid Loss: 3300.19780, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3316.59980, Valid Loss: 3299.30705, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3315.38720, Valid Loss: 3310.00839, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3314.59211, Valid Loss: 3293.48633, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3303.59603, Valid Loss: 3301.43338, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3299.06570, Valid Loss: 3285.92173, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3286.68870, Valid Loss: 3261.71441, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3274.70221, Valid Loss: 3232.34170, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3263.73675, Valid Loss: 3257.12297, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3260.45107, Valid Loss: 3222.58871, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3254.02234, Valid Loss: 3226.52788, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3254.46819, Valid Loss: 3220.54157, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3249.54572, Valid Loss: 3216.90966, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3259.01583, Valid Loss: 3239.98574, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3248.13010, Valid Loss: 3214.33621, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3247.02071, Valid Loss: 3230.90830, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3243.44251, Valid Loss: 3216.08881, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3245.94597, Valid Loss: 3219.21289, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "from udlp.clustering.vade import VaDE\n",
    "args_pretrain = 'model/pre_vade.pt'\n",
    "\n",
    "vade = VaDE(input_dim=in_features, z_dim=10, n_centroids=10, binary=True,\n",
    "        encodeLayer=[2000,500,2000], decodeLayer=[2000,500,2000])\n",
    "\n",
    "# if args_pretrain != \"\":\n",
    "#     print(\"Loading model from %s...\" % args_pretrain)\n",
    "#     vade.load_model(args_pretrain)\n",
    "print(\"Initializing through GMM..\")\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: nan\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 6242.55512, Valid Loss: 3307.95430, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3317.17495, Valid Loss: 3339.55438, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3308.81975, Valid Loss: 3289.09791, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3303.23821, Valid Loss: 3262.72100, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3311.81923, Valid Loss: 3269.18336, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3293.82434, Valid Loss: 3321.69122, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3287.29481, Valid Loss: 3243.17547, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3272.09096, Valid Loss: 3257.97516, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3269.02134, Valid Loss: 3237.02857, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3261.04076, Valid Loss: 3233.40067, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3260.28980, Valid Loss: 3232.83958, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3259.21324, Valid Loss: 3241.96530, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3255.28221, Valid Loss: 3275.28356, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3252.22135, Valid Loss: 3222.59020, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3248.78229, Valid Loss: 3209.03423, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3241.07461, Valid Loss: 3221.75984, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3231.41034, Valid Loss: 3199.08643, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3230.16210, Valid Loss: 3208.34866, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3231.20179, Valid Loss: 3203.62217, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3226.46610, Valid Loss: 3208.21964, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "from udlp.clustering.vade import VaDE\n",
    "args_pretrain = 'model/pre_vade.pt'\n",
    "\n",
    "z_dim = 20\n",
    "n_centroids = 20\n",
    "vade = VaDE(input_dim=in_features, z_dim=z_dim, n_centroids=n_centroids, binary=True,\n",
    "        encodeLayer=[2000,500,2000], decodeLayer=[2000,500,2000])\n",
    "\n",
    "# if args_pretrain != \"\":\n",
    "#     print(\"Loading model from %s...\" % args_pretrain)\n",
    "#     vade.load_model(args_pretrain)\n",
    "print(\"Initializing through GMM..\")\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.save_model(\"model/vade2k5c2k_2020_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Epoch -1: Valid Loss: nan\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3311.62904, Valid Loss: 3223.40162, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1772:\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 15, in _jload\n",
      "    img_data = img.get_data()\n",
      "Process Process-1771:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 39, in __getitem__\n",
      "    img_data = PacDataset._jload(file_id)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/dataobj_images.py\", line 202, in get_data\n",
      "    data = np.asanyarray(self._dataobj)\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 39, in __getitem__\n",
      "    img_data = PacDataset._jload(file_id)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/numeric.py\", line 544, in asanyarray\n",
      "    return array(a, dtype, copy=False, order=order, subok=True)\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 15, in _jload\n",
      "    img_data = img.get_data()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/arrayproxy.py\", line 291, in __array__\n",
      "    return apply_read_scaling(raw_data, self._slope, self._inter)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-21d91cb7ea34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model/vade2k5c2k_2020_1.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manneal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/UnsupervisedDeepLearning-Pytorch/udlp/clustering/vade.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainloader, validloader, lr, batch_size, num_epochs, visualize, anneal)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mvalid_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;31m# total_loss += valid_recon_loss.data[0] * inputs.size()[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/UnsupervisedDeepLearning-Pytorch/udlp/clustering/vade.py\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(self, recon_x, x, z, z_mean, z_log_var)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_c_z\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_c_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NxK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         BCE = -torch.sum(x*torch.log(torch.clamp(recon_x, min=1e-10))+\n\u001b[0m\u001b[1;32m    165\u001b[0m             (1-x)*torch.log(torch.clamp(1-recon_x, min=1e-10)), 1)\n\u001b[1;32m    166\u001b[0m         logpzc = torch.sum(0.5*gamma*torch.sum(math.log(2*math.pi)+torch.log(lambda_tensor3)+\\\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mclamp\u001b[0;34m(self, min, max)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ByteTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmax\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             raise ValueError(\"clamp requires specifying at least one of \"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/dataobj_images.py\", line 202, in get_data\n",
      "    data = np.asanyarray(self._dataobj)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/volumeutils.py\", line 965, in apply_read_scaling\n",
      "    arr = arr * slope\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/numeric.py\", line 544, in asanyarray\n",
      "    return array(a, dtype, copy=False, order=order, subok=True)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/memmap.py\", line 319, in __array_wrap__\n",
      "    def __array_wrap__(self, arr, context=None):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/arrayproxy.py\", line 291, in __array__\n",
      "    return apply_read_scaling(raw_data, self._slope, self._inter)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/volumeutils.py\", line 965, in apply_read_scaling\n",
      "    arr = arr * slope\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/memmap.py\", line 319, in __array_wrap__\n",
      "    def __array_wrap__(self, arr, context=None):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "vade.load_model(\"model/vade2k5c2k_2020_1.pt\")\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.save_model(\"model/vade2k5c2k_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 6526.86497\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3262.76987, Valid Loss: 3232.34285, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3242.06520, Valid Loss: 3211.50148, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3242.81475, Valid Loss: 3210.97557, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3236.35666, Valid Loss: 3204.28867, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3235.02464, Valid Loss: 3200.39091, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3247.62324, Valid Loss: 3212.14897, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3234.69476, Valid Loss: 3191.59323, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3224.14547, Valid Loss: 3188.05998, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3223.49146, Valid Loss: 3187.75288, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3220.72840, Valid Loss: 3191.08219, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3217.33778, Valid Loss: 3184.41324, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3215.51416, Valid Loss: 3186.50432, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3219.40033, Valid Loss: 3190.32580, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3216.65963, Valid Loss: 3200.23926, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3215.32177, Valid Loss: 3193.80520, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3215.23825, Valid Loss: 3197.35659, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3220.81869, Valid Loss: 3197.52757, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3216.14536, Valid Loss: 3184.32798, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3211.68744, Valid Loss: 3184.49495, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3211.72615, Valid Loss: 3183.83085, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "vade.load_model(\"model/vade2k5c2k_1.pt\")\n",
    "print(\"Initializing through GMM..\")\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.save_model(\"model/vade2k5c2k_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 8679.95242\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3233.41579, Valid Loss: 3186.49018, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3209.28563, Valid Loss: 3183.59263, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3211.07545, Valid Loss: 3183.09875, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3211.02946, Valid Loss: 3182.68724, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3211.01418, Valid Loss: 3182.51615, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3209.16660, Valid Loss: 3183.76033, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3208.18500, Valid Loss: 3181.53952, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3210.69156, Valid Loss: 3182.22657, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3216.56231, Valid Loss: 3192.80368, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3228.06418, Valid Loss: 3204.26479, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3213.43192, Valid Loss: 3181.38049, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3207.09618, Valid Loss: 3184.59422, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3207.15144, Valid Loss: 3187.67181, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3203.35760, Valid Loss: 3182.49932, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3207.64340, Valid Loss: 3183.93345, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3201.76650, Valid Loss: 3181.46259, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3201.96666, Valid Loss: 3199.70138, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3207.04547, Valid Loss: 3184.02719, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3201.83120, Valid Loss: 3181.26704, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3199.76877, Valid Loss: 3181.12559, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "vade.load_model(\"model/vade2k5c2k_2.pt\")\n",
    "print(\"Initializing through GMM..\")\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.save_model(\"model/vade2k5c2k_3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 13186.21874\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3230.66995, Valid Loss: 3191.42460, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3198.93020, Valid Loss: 3182.39906, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3200.67083, Valid Loss: 3181.66593, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3199.40258, Valid Loss: 3183.60423, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3195.83137, Valid Loss: 3179.73581, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3197.30092, Valid Loss: 3181.60865, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3194.53059, Valid Loss: 3180.47269, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3202.62261, Valid Loss: 3182.16113, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3194.71276, Valid Loss: 3183.98300, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3197.05575, Valid Loss: 3185.75998, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3194.80184, Valid Loss: 3179.34724, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3191.17852, Valid Loss: 3181.93699, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3191.43532, Valid Loss: 3198.15559, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3195.77549, Valid Loss: 3180.40157, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3193.38911, Valid Loss: 3185.34664, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3194.80324, Valid Loss: 3184.20453, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3192.44465, Valid Loss: 3182.01015, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3190.33184, Valid Loss: 3187.10710, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3191.58419, Valid Loss: 3181.00482, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3196.09385, Valid Loss: 3179.97391, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "vade.load_model(\"model/vade2k5c2k_3.pt\")\n",
    "print(\"Initializing through GMM..\")\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I don't know the number of centroids does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 3219.21289\n",
      "#Epoch   0: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1496:\n",
      "Process Process-1495:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 38, in __getitem__\n",
      "    file_id = self.train0_df.iloc[idx]['file_id']\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/indexing.py\", line 1373, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/indexing.py\", line 1832, in _getitem_axis\n",
      "    return self._get_loc(key, axis=axis)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/indexing.py\", line 150, in _get_loc\n",
      "    return self.obj._ixs(key, axis=axis)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/frame.py\", line 2074, in _ixs\n",
      "    dtype=new_values.dtype)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/series.py\", line 264, in __init__\n",
      "    raise_cast_failure=True)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/series.py\", line 3178, in _sanitize_array\n",
      "    subarr = np.array(data, copy=False)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7fecceac2f98>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 315, in _shutdown_workers\n",
      "    self.shutdown = True\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 10104) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-34cb06e28dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# vade.initialize_gmm(train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manneal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/UnsupervisedDeepLearning-Pytorch/udlp/clustering/vade.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainloader, validloader, lr, batch_size, num_epochs, visualize, anneal)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vade.load_model(\"model/vade2k5c2k_1.pt\")\n",
    "print(\"Initializing through GMM..\")\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "# we do NOT want to initialize_gmm each time, oh wait, yes we do.\n",
    "# vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Epoch -1: Valid Loss: 3208.37063\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3107.86518, Valid Loss: 3172.76114, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3110.91332, Valid Loss: 3176.45886, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3110.19600, Valid Loss: 3178.79659, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3104.94653, Valid Loss: 3183.62467, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3118.62083, Valid Loss: 3178.66328, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3114.10822, Valid Loss: 3199.02968, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3120.37661, Valid Loss: 3173.95579, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3107.35450, Valid Loss: 3185.31540, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3095.57662, Valid Loss: 3168.91423, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3088.22531, Valid Loss: 3190.91982, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3084.56503, Valid Loss: 3168.53700, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3090.34077, Valid Loss: 3179.14629, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3080.38083, Valid Loss: 3175.84292, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3081.48360, Valid Loss: 3183.20560, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3086.17329, Valid Loss: 3169.25121, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3101.88350, Valid Loss: 3204.75258, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3145.60033, Valid Loss: 3211.37616, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3093.38880, Valid Loss: 3174.89950, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3086.31131, Valid Loss: 3180.41810, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3106.79093, Valid Loss: 3176.52110, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 8\n",
    "# vade.load_model(\"model/vade1.pt\")\n",
    "# print(\"Initializing through GMM..\")\n",
    "# vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
