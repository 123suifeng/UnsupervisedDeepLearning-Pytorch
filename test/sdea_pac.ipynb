{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import torch.utils.data\n",
    "# from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import argparse\n",
    "from udlp.autoencoder.denoisingAutoencoder import DenoisingAutoencoder\n",
    "from pacdataset import PacDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "lr = 0.001\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEA with PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"/Users/josephmann/Documents/Gheiratmand/sMRI competition/PAC Data/pac2018/\"\n",
    "datasets = {x: PacDataset(train=(x=='train'), \n",
    "                          root_dir = \"/home/paperspace/data/pac2018/\") \n",
    "#                           root_dir = \"/Users/josephmann/Documents/Gheiratmand/sMRI competition/PAC Data/pac2018/\") \n",
    "            for x in ['val','train']}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets['train'],\n",
    "    batch_size= batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets['val'],\n",
    "    batch_size= batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Denoising Autoencoding layer=======\n",
      "####Epoch 0: Valid Reconstruct Loss: 12169.798\n",
      "#Epoch   1: Reconstruct Loss: 3520.461, Valid Reconstruct Loss: 3417.626\n",
      "#Epoch   2: Reconstruct Loss: 3334.102, Valid Reconstruct Loss: 3370.125\n"
     ]
    }
   ],
   "source": [
    "in_features = 17545\n",
    "out_features = 500\n",
    "dae = DenoisingAutoencoder(in_features, out_features)\n",
    "dae.fit(train_loader, test_loader, lr=lr, num_epochs=epochs, loss_type=\"cross-entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDEA with PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from udlp.autoencoder.stackedDAE import StackedDAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l is  1\n",
      "=====Denoising Autoencoding layer=======\n",
      "####Epoch 0: Valid Reconstruct Loss: 12172.528\n",
      "#Epoch   1: Reconstruct Loss: 3411.094, Valid Reconstruct Loss: 3393.106\n",
      "#Epoch   2: Reconstruct Loss: 3278.276, Valid Reconstruct Loss: 3316.640\n",
      "#Epoch   3: Reconstruct Loss: 3241.960, Valid Reconstruct Loss: 3299.227\n",
      "#Epoch   4: Reconstruct Loss: 3206.112, Valid Reconstruct Loss: 3261.768\n",
      "#Epoch   5: Reconstruct Loss: 3180.514, Valid Reconstruct Loss: 3263.220\n",
      "#Epoch   6: Reconstruct Loss: 3156.363, Valid Reconstruct Loss: 3211.948\n",
      "#Epoch   7: Reconstruct Loss: 3131.904, Valid Reconstruct Loss: 3188.156\n",
      "#Epoch   8: Reconstruct Loss: 3112.562, Valid Reconstruct Loss: 3210.073\n",
      "#Epoch   9: Reconstruct Loss: 3095.244, Valid Reconstruct Loss: 3185.712\n",
      "#Epoch  10: Reconstruct Loss: 3080.548, Valid Reconstruct Loss: 3178.328\n",
      "#Epoch  11: Reconstruct Loss: 3066.563, Valid Reconstruct Loss: 3154.954\n",
      "#Epoch  12: Reconstruct Loss: 3052.418, Valid Reconstruct Loss: 3166.015\n",
      "#Epoch  13: Reconstruct Loss: 3042.353, Valid Reconstruct Loss: 3134.499\n",
      "#Epoch  14: Reconstruct Loss: 3034.922, Valid Reconstruct Loss: 3162.605\n",
      "#Epoch  15: Reconstruct Loss: 3024.551, Valid Reconstruct Loss: 3149.663\n",
      "#Epoch  16: Reconstruct Loss: 3015.833, Valid Reconstruct Loss: 3151.142\n",
      "#Epoch  17: Reconstruct Loss: 3008.929, Valid Reconstruct Loss: 3162.194\n",
      "#Epoch  18: Reconstruct Loss: 3004.982, Valid Reconstruct Loss: 3151.265\n",
      "#Epoch  19: Reconstruct Loss: 2998.064, Valid Reconstruct Loss: 3149.551\n",
      "#Epoch  20: Reconstruct Loss: 2993.390, Valid Reconstruct Loss: 3142.254\n",
      "type(data_x) <class 'torch.FloatTensor'>\n",
      "type(valid_x) <class 'torch.FloatTensor'>\n",
      "l is  2\n",
      "=====Denoising Autoencoding layer=======\n",
      "####Epoch 0: Valid Reconstruct Loss: 1823.721\n",
      "#Epoch   1: Reconstruct Loss: 1055.667, Valid Reconstruct Loss: 778.597\n",
      "#Epoch   2: Reconstruct Loss: 722.897, Valid Reconstruct Loss: 633.731\n",
      "#Epoch   3: Reconstruct Loss: 628.013, Valid Reconstruct Loss: 523.709\n",
      "#Epoch   4: Reconstruct Loss: 549.722, Valid Reconstruct Loss: 494.467\n",
      "#Epoch   5: Reconstruct Loss: 497.999, Valid Reconstruct Loss: 441.005\n",
      "#Epoch   6: Reconstruct Loss: 453.653, Valid Reconstruct Loss: 410.582\n",
      "#Epoch   7: Reconstruct Loss: 419.540, Valid Reconstruct Loss: 351.512\n",
      "#Epoch   8: Reconstruct Loss: 393.542, Valid Reconstruct Loss: 370.381\n",
      "#Epoch   9: Reconstruct Loss: 374.963, Valid Reconstruct Loss: 330.557\n",
      "#Epoch  10: Reconstruct Loss: 361.309, Valid Reconstruct Loss: 327.626\n",
      "#Epoch  11: Reconstruct Loss: 351.336, Valid Reconstruct Loss: 284.544\n",
      "#Epoch  12: Reconstruct Loss: 344.538, Valid Reconstruct Loss: 350.426\n",
      "#Epoch  13: Reconstruct Loss: 334.548, Valid Reconstruct Loss: 298.068\n",
      "#Epoch  14: Reconstruct Loss: 328.624, Valid Reconstruct Loss: 287.021\n",
      "#Epoch  15: Reconstruct Loss: 322.712, Valid Reconstruct Loss: 257.611\n",
      "#Epoch  16: Reconstruct Loss: 319.595, Valid Reconstruct Loss: 293.807\n",
      "#Epoch  17: Reconstruct Loss: 319.391, Valid Reconstruct Loss: 288.329\n",
      "#Epoch  18: Reconstruct Loss: 311.984, Valid Reconstruct Loss: 294.885\n",
      "#Epoch  19: Reconstruct Loss: 312.490, Valid Reconstruct Loss: 272.957\n",
      "#Epoch  20: Reconstruct Loss: 310.460, Valid Reconstruct Loss: 270.584\n",
      "type(data_x) <class 'torch.FloatTensor'>\n",
      "type(valid_x) <class 'torch.FloatTensor'>\n",
      "l is  3\n",
      "=====Denoising Autoencoding layer=======\n",
      "####Epoch 0: Valid Reconstruct Loss: 2345.873\n",
      "#Epoch   1: Reconstruct Loss: 792.516, Valid Reconstruct Loss: 666.350\n",
      "#Epoch   2: Reconstruct Loss: 507.179, Valid Reconstruct Loss: 588.306\n",
      "#Epoch   3: Reconstruct Loss: 456.636, Valid Reconstruct Loss: 567.677\n",
      "#Epoch   4: Reconstruct Loss: 393.691, Valid Reconstruct Loss: 509.381\n",
      "#Epoch   5: Reconstruct Loss: 341.772, Valid Reconstruct Loss: 469.986\n",
      "#Epoch   6: Reconstruct Loss: 300.027, Valid Reconstruct Loss: 509.420\n",
      "#Epoch   7: Reconstruct Loss: 267.105, Valid Reconstruct Loss: 443.811\n",
      "#Epoch   8: Reconstruct Loss: 240.905, Valid Reconstruct Loss: 446.460\n",
      "#Epoch   9: Reconstruct Loss: 218.980, Valid Reconstruct Loss: 441.683\n",
      "#Epoch  10: Reconstruct Loss: 203.363, Valid Reconstruct Loss: 425.701\n",
      "#Epoch  11: Reconstruct Loss: 190.779, Valid Reconstruct Loss: 428.006\n",
      "#Epoch  12: Reconstruct Loss: 177.626, Valid Reconstruct Loss: 407.617\n",
      "#Epoch  13: Reconstruct Loss: 168.883, Valid Reconstruct Loss: 394.442\n",
      "#Epoch  14: Reconstruct Loss: 162.201, Valid Reconstruct Loss: 403.546\n",
      "#Epoch  15: Reconstruct Loss: 155.554, Valid Reconstruct Loss: 402.730\n",
      "#Epoch  16: Reconstruct Loss: 150.020, Valid Reconstruct Loss: 365.182\n",
      "#Epoch  17: Reconstruct Loss: 145.933, Valid Reconstruct Loss: 470.534\n",
      "#Epoch  18: Reconstruct Loss: 143.532, Valid Reconstruct Loss: 417.848\n",
      "#Epoch  19: Reconstruct Loss: 137.726, Valid Reconstruct Loss: 381.648\n",
      "#Epoch  20: Reconstruct Loss: 134.472, Valid Reconstruct Loss: 374.899\n",
      "type(data_x) <class 'torch.FloatTensor'>\n",
      "type(valid_x) <class 'torch.FloatTensor'>\n",
      "l is  4\n",
      "=====Denoising Autoencoding layer=======\n",
      "####Epoch 0: Valid Reconstruct Loss: 2002.966\n",
      "#Epoch   1: Reconstruct Loss: 1186.373, Valid Reconstruct Loss: 617.704\n",
      "#Epoch   2: Reconstruct Loss: 476.675, Valid Reconstruct Loss: 580.086\n",
      "#Epoch   3: Reconstruct Loss: 451.194, Valid Reconstruct Loss: 551.716\n",
      "#Epoch   4: Reconstruct Loss: 432.672, Valid Reconstruct Loss: 536.284\n",
      "#Epoch   5: Reconstruct Loss: 412.337, Valid Reconstruct Loss: 549.030\n",
      "#Epoch   6: Reconstruct Loss: 390.845, Valid Reconstruct Loss: 519.610\n",
      "#Epoch   7: Reconstruct Loss: 368.851, Valid Reconstruct Loss: 485.187\n",
      "#Epoch   8: Reconstruct Loss: 346.965, Valid Reconstruct Loss: 469.700\n",
      "#Epoch   9: Reconstruct Loss: 324.646, Valid Reconstruct Loss: 454.980\n",
      "#Epoch  10: Reconstruct Loss: 305.596, Valid Reconstruct Loss: 464.713\n",
      "#Epoch  11: Reconstruct Loss: 287.483, Valid Reconstruct Loss: 410.056\n",
      "#Epoch  12: Reconstruct Loss: 273.429, Valid Reconstruct Loss: 414.497\n",
      "#Epoch  13: Reconstruct Loss: 261.925, Valid Reconstruct Loss: 429.679\n",
      "#Epoch  14: Reconstruct Loss: 252.672, Valid Reconstruct Loss: 427.255\n",
      "#Epoch  15: Reconstruct Loss: 241.449, Valid Reconstruct Loss: 399.267\n",
      "#Epoch  16: Reconstruct Loss: 236.095, Valid Reconstruct Loss: 399.583\n",
      "#Epoch  17: Reconstruct Loss: 227.935, Valid Reconstruct Loss: 373.696\n",
      "#Epoch  18: Reconstruct Loss: 221.963, Valid Reconstruct Loss: 381.619\n",
      "#Epoch  19: Reconstruct Loss: 214.525, Valid Reconstruct Loss: 362.533\n",
      "#Epoch  20: Reconstruct Loss: 210.991, Valid Reconstruct Loss: 386.291\n",
      "type(data_x) <class 'torch.FloatTensor'>\n",
      "type(valid_x) <class 'torch.FloatTensor'>\n",
      "=====Stacked Denoising Autoencoding layer=======\n",
      "#Epoch 0: Valid Reconstruct Loss: 6615.045\n",
      "#Epoch   1: Reconstruct Loss: 3182.542, Valid Reconstruct Loss: 3189.980\n",
      "#Epoch   2: Reconstruct Loss: 3137.881, Valid Reconstruct Loss: 3188.891\n",
      "#Epoch   3: Reconstruct Loss: 3122.276, Valid Reconstruct Loss: 3220.256\n",
      "#Epoch   4: Reconstruct Loss: 3111.500, Valid Reconstruct Loss: 3259.575\n",
      "#Epoch   5: Reconstruct Loss: 3106.181, Valid Reconstruct Loss: 3201.045\n",
      "#Epoch   6: Reconstruct Loss: 3098.192, Valid Reconstruct Loss: 3178.643\n",
      "#Epoch   7: Reconstruct Loss: 3088.211, Valid Reconstruct Loss: 3180.974\n",
      "#Epoch   8: Reconstruct Loss: 3083.056, Valid Reconstruct Loss: 3180.168\n",
      "#Epoch   9: Reconstruct Loss: 3074.065, Valid Reconstruct Loss: 3162.201\n",
      "#Epoch  10: Reconstruct Loss: 3067.599, Valid Reconstruct Loss: 3155.286\n",
      "#Epoch  11: Reconstruct Loss: 3058.566, Valid Reconstruct Loss: 3158.833\n",
      "#Epoch  12: Reconstruct Loss: 3057.786, Valid Reconstruct Loss: 3162.302\n",
      "#Epoch  13: Reconstruct Loss: 3052.322, Valid Reconstruct Loss: 3172.475\n",
      "#Epoch  14: Reconstruct Loss: 3041.923, Valid Reconstruct Loss: 3155.455\n",
      "#Epoch  15: Reconstruct Loss: 3033.599, Valid Reconstruct Loss: 3152.489\n",
      "#Epoch  16: Reconstruct Loss: 3027.037, Valid Reconstruct Loss: 3161.187\n",
      "#Epoch  17: Reconstruct Loss: 3023.491, Valid Reconstruct Loss: 3169.915\n",
      "#Epoch  18: Reconstruct Loss: 3014.249, Valid Reconstruct Loss: 3173.541\n",
      "#Epoch  19: Reconstruct Loss: 3006.172, Valid Reconstruct Loss: 3155.347\n",
      "#Epoch  20: Reconstruct Loss: 3004.599, Valid Reconstruct Loss: 3171.951\n"
     ]
    }
   ],
   "source": [
    "# in_features = 784\n",
    "in_features = 17545\n",
    "out_features = 500\n",
    "pretrainepochs = 20 \n",
    "epochs = 20\n",
    "z_dim = 100\n",
    "batch_size = 32\n",
    "\n",
    "sdae = StackedDAE(input_dim=in_features, z_dim=z_dim, binary=True,\n",
    "    encodeLayer=[4000,1000,500], decodeLayer=[500,1000,4000], activation=\"relu\", \n",
    "    dropout=0)\n",
    "sdae.pretrain(train_loader, test_loader, lr=lr, batch_size= batch_size, \n",
    "    num_epochs=pretrainepochs, corrupt=0.3, loss_type=\"cross-entropy\")\n",
    "sdae.save_model(\"model/sdae.pt\")\n",
    "sdae.fit(train_loader, test_loader, lr= lr, num_epochs= epochs, corrupt=0.3, loss_type=\"cross-entropy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdae.save_model(\"model/pre_vade.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Stacked Denoising Autoencoding layer=======\n",
      "#Epoch 0: Valid Reconstruct Loss: 3171.951\n",
      "#Epoch   1: Reconstruct Loss: 2987.999, Valid Reconstruct Loss: 3159.586\n",
      "#Epoch   2: Reconstruct Loss: 2972.008, Valid Reconstruct Loss: 3149.414\n",
      "#Epoch   3: Reconstruct Loss: 2972.544, Valid Reconstruct Loss: 3163.527\n",
      "#Epoch   4: Reconstruct Loss: 2960.754, Valid Reconstruct Loss: 3176.698\n",
      "#Epoch   5: Reconstruct Loss: 2961.723, Valid Reconstruct Loss: 3199.294\n",
      "#Epoch   6: Reconstruct Loss: 2940.072, Valid Reconstruct Loss: 3162.047\n",
      "#Epoch   7: Reconstruct Loss: 2934.605, Valid Reconstruct Loss: 3173.188\n",
      "#Epoch   8: Reconstruct Loss: 2905.482, Valid Reconstruct Loss: 3163.828\n",
      "#Epoch   9: Reconstruct Loss: 2898.027, Valid Reconstruct Loss: 3159.685\n",
      "#Epoch  10: Reconstruct Loss: 2888.576, Valid Reconstruct Loss: 3177.659\n",
      "#Epoch  11: Reconstruct Loss: 2870.851, Valid Reconstruct Loss: 3178.913\n",
      "#Epoch  12: Reconstruct Loss: 2850.923, Valid Reconstruct Loss: 3175.868\n",
      "#Epoch  13: Reconstruct Loss: 2840.232, Valid Reconstruct Loss: 3170.975\n",
      "#Epoch  14: Reconstruct Loss: 2844.835, Valid Reconstruct Loss: 3179.360\n",
      "#Epoch  15: Reconstruct Loss: 2836.757, Valid Reconstruct Loss: 3179.741\n",
      "#Epoch  16: Reconstruct Loss: 2816.399, Valid Reconstruct Loss: 3179.079\n",
      "#Epoch  17: Reconstruct Loss: 2804.320, Valid Reconstruct Loss: 3205.497\n",
      "#Epoch  18: Reconstruct Loss: 2798.093, Valid Reconstruct Loss: 3185.049\n",
      "#Epoch  19: Reconstruct Loss: 2768.230, Valid Reconstruct Loss: 3188.732\n",
      "#Epoch  20: Reconstruct Loss: 2760.185, Valid Reconstruct Loss: 3191.493\n"
     ]
    }
   ],
   "source": [
    "sdae.fit(train_loader, test_loader, lr= lr, num_epochs= epochs, corrupt=0.3, loss_type=\"cross-entropy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a reconstruction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch at /opt/conda/conda-bld/pytorch_1522182087074/work/torch/lib/THC/generic/THCTensorMathBlas.cu:247",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a0900bb99f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mf_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msdae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m145\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/UnsupervisedDeepLearning-Pytorch/udlp/autoencoder/stackedDAE.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enc_mu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch at /opt/conda/conda-bld/pytorch_1522182087074/work/torch/lib/THC/generic/THCTensorMathBlas.cu:247"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf057a1a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n",
    "#                                     root_dir='faces/')\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "for i in range(4): #len(datasets[0])):\n",
    "    inputs = datasets['train'][i]\n",
    "    inputs = inputs.view(inputs.size(0), -1).float()\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    inputs = Variable(inputs)\n",
    "    z, outputs = sdae.forward(inputs)\n",
    "    f_sample = sdae.forward(datasets['train'][i])\n",
    "    sample = datasets['train'][i].numpy().reshape(121,145)\n",
    "\n",
    "    print(i, sample.shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "#     plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(sample)\n",
    "#     show_landmarks(**sample)\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdae."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test VaDE with PAC and pre-trained model\n",
    "just for file format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from model/pre_vade.pt...\n",
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 4140.25597\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3241.66117, Valid Loss: 3192.90484, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3184.89950, Valid Loss: 3206.42785, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3169.68459, Valid Loss: 3202.02557, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3170.44605, Valid Loss: 3185.52840, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3162.35217, Valid Loss: 3196.19272, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3171.72202, Valid Loss: 3184.59456, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3168.05140, Valid Loss: 3174.83904, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3158.79186, Valid Loss: 3212.82876, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3151.82832, Valid Loss: 3219.81215, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3155.57900, Valid Loss: 3219.58857, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3164.65678, Valid Loss: 3181.88866, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3145.01423, Valid Loss: 3190.64643, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3143.73049, Valid Loss: 3195.79738, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3160.15543, Valid Loss: 3191.97362, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3149.24420, Valid Loss: 3184.50306, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3136.62730, Valid Loss: 3189.83244, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3135.73869, Valid Loss: 3174.86144, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3137.64300, Valid Loss: 3180.55859, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3131.77725, Valid Loss: 3174.23543, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3127.84978, Valid Loss: 3177.00404, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "from udlp.clustering.vade import VaDE\n",
    "args_pretrain = 'model/pre_vade.pt'\n",
    "\n",
    "vade = VaDE(input_dim=in_features, z_dim=10, n_centroids=10, binary=True,\n",
    "        encodeLayer=[500,500,2000], decodeLayer=[2000,500,500])\n",
    "\n",
    "if args_pretrain != \"\":\n",
    "    print(\"Loading model from %s...\" % args_pretrain)\n",
    "    vade.load_model(args_pretrain)\n",
    "print(\"Initializing through GMM..\")\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.save_model(\"model/vade2k5c5c_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.load_model(\"model/vade1.pt\")\n",
    "print(\"Initializing through GMM..\")\n",
    "batch_size = 32\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from model/pre_vade.pt...\n",
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 4322.98422\n",
      "#Epoch   0: lr: 0.00050, Train Loss: 3345.01400, Valid Loss: 3185.95488, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00050, Train Loss: 3186.08449, Valid Loss: 3180.28178, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00050, Train Loss: 3179.28265, Valid Loss: 3180.05393, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00050, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00050, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-509:\n",
      "Process Process-510:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7fcf10410c88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-df03fb097773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing through GMM..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mvade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mvade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manneal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/UnsupervisedDeepLearning-Pytorch/udlp/clustering/vade.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainloader, validloader, lr, batch_size, num_epochs, visualize, anneal)\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 39, in __getitem__\n",
      "    img_data = PacDataset._jload(file_id)\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 15, in _jload\n",
      "    img_data = img.get_data()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/dataobj_images.py\", line 202, in get_data\n",
      "    data = np.asanyarray(self._dataobj)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/numeric.py\", line 544, in asanyarray\n",
      "    return array(a, dtype, copy=False, order=order, subok=True)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/arrayproxy.py\", line 291, in __array__\n",
      "    return apply_read_scaling(raw_data, self._slope, self._inter)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/volumeutils.py\", line 961, in apply_read_scaling\n",
      "    ftype = int_scinter_ftype(arr.dtype, slope, inter, default)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/volumeutils.py\", line 1209, in int_scinter_ftype\n",
      "    return _ftype4scaled_finite(tst_arr, slope, inter, 'read', default)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/volumeutils.py\", line 1353, in _ftype4scaled_finite\n",
      "    if np.all(np.isfinite(tst_trans)):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/fromnumeric.py\", line 2098, in all\n",
      "    return arr.all(axis=axis, out=out, **kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/_methods.py\", line 41, in _all\n",
      "    return umr_all(a, axis, dtype, out, keepdims)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from udlp.clustering.vade import VaDE\n",
    "args_pretrain = 'model/pre_vade.pt'\n",
    "\n",
    "vade = VaDE(input_dim=in_features, z_dim=20, n_centroids=50, binary=True,\n",
    "        encodeLayer=[2000,500,2000], decodeLayer=[2000,500,2000])\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "if args_pretrain != \"\":\n",
    "    print(\"Loading model from %s...\" % args_pretrain)\n",
    "    vade.load_model(args_pretrain)\n",
    "print(\"Initializing through GMM..\")\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: nan\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 6242.55512, Valid Loss: 3307.95430, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3317.17495, Valid Loss: 3339.55438, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3308.81975, Valid Loss: 3289.09791, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3303.23821, Valid Loss: 3262.72100, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3311.81923, Valid Loss: 3269.18336, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3293.82434, Valid Loss: 3321.69122, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3287.29481, Valid Loss: 3243.17547, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3272.09096, Valid Loss: 3257.97516, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3269.02134, Valid Loss: 3237.02857, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3261.04076, Valid Loss: 3233.40067, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3260.28980, Valid Loss: 3232.83958, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3259.21324, Valid Loss: 3241.96530, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3255.28221, Valid Loss: 3275.28356, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3252.22135, Valid Loss: 3222.59020, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3248.78229, Valid Loss: 3209.03423, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3241.07461, Valid Loss: 3221.75984, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3231.41034, Valid Loss: 3199.08643, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3230.16210, Valid Loss: 3208.34866, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3231.20179, Valid Loss: 3203.62217, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3226.46610, Valid Loss: 3208.21964, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "from udlp.clustering.vade import VaDE\n",
    "args_pretrain = 'model/pre_vade.pt'\n",
    "\n",
    "z_dim = 20\n",
    "n_centroids = 20\n",
    "vade = VaDE(input_dim=in_features, z_dim=z_dim, n_centroids=n_centroids, binary=True,\n",
    "        encodeLayer=[2000,500,2000], decodeLayer=[2000,500,2000])\n",
    "\n",
    "# if args_pretrain != \"\":\n",
    "#     print(\"Loading model from %s...\" % args_pretrain)\n",
    "#     vade.load_model(args_pretrain)\n",
    "print(\"Initializing through GMM..\")\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.save_model(\"model/vade2k5c2k_2020_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Epoch -1: Valid Loss: nan\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3311.62904, Valid Loss: 3223.40162, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1772:\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 15, in _jload\n",
      "    img_data = img.get_data()\n",
      "Process Process-1771:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 39, in __getitem__\n",
      "    img_data = PacDataset._jload(file_id)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/dataobj_images.py\", line 202, in get_data\n",
      "    data = np.asanyarray(self._dataobj)\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 39, in __getitem__\n",
      "    img_data = PacDataset._jload(file_id)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/numeric.py\", line 544, in asanyarray\n",
      "    return array(a, dtype, copy=False, order=order, subok=True)\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 15, in _jload\n",
      "    img_data = img.get_data()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/arrayproxy.py\", line 291, in __array__\n",
      "    return apply_read_scaling(raw_data, self._slope, self._inter)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-21d91cb7ea34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model/vade2k5c2k_2020_1.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manneal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/UnsupervisedDeepLearning-Pytorch/udlp/clustering/vade.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainloader, validloader, lr, batch_size, num_epochs, visualize, anneal)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mvalid_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;31m# total_loss += valid_recon_loss.data[0] * inputs.size()[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/UnsupervisedDeepLearning-Pytorch/udlp/clustering/vade.py\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(self, recon_x, x, z, z_mean, z_log_var)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_c_z\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_c_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NxK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         BCE = -torch.sum(x*torch.log(torch.clamp(recon_x, min=1e-10))+\n\u001b[0m\u001b[1;32m    165\u001b[0m             (1-x)*torch.log(torch.clamp(1-recon_x, min=1e-10)), 1)\n\u001b[1;32m    166\u001b[0m         logpzc = torch.sum(0.5*gamma*torch.sum(math.log(2*math.pi)+torch.log(lambda_tensor3)+\\\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mclamp\u001b[0;34m(self, min, max)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ByteTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmax\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             raise ValueError(\"clamp requires specifying at least one of \"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/dataobj_images.py\", line 202, in get_data\n",
      "    data = np.asanyarray(self._dataobj)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/volumeutils.py\", line 965, in apply_read_scaling\n",
      "    arr = arr * slope\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/numeric.py\", line 544, in asanyarray\n",
      "    return array(a, dtype, copy=False, order=order, subok=True)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/memmap.py\", line 319, in __array_wrap__\n",
      "    def __array_wrap__(self, arr, context=None):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/arrayproxy.py\", line 291, in __array__\n",
      "    return apply_read_scaling(raw_data, self._slope, self._inter)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/nibabel/volumeutils.py\", line 965, in apply_read_scaling\n",
      "    arr = arr * slope\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/core/memmap.py\", line 319, in __array_wrap__\n",
      "    def __array_wrap__(self, arr, context=None):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "vade.load_model(\"model/vade2k5c2k_2020_1.pt\")\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.save_model(\"model/vade2k5c2k_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 6526.86497\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3262.76987, Valid Loss: 3232.34285, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3242.06520, Valid Loss: 3211.50148, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3242.81475, Valid Loss: 3210.97557, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3236.35666, Valid Loss: 3204.28867, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3235.02464, Valid Loss: 3200.39091, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3247.62324, Valid Loss: 3212.14897, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3234.69476, Valid Loss: 3191.59323, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3224.14547, Valid Loss: 3188.05998, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3223.49146, Valid Loss: 3187.75288, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3220.72840, Valid Loss: 3191.08219, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3217.33778, Valid Loss: 3184.41324, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3215.51416, Valid Loss: 3186.50432, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3219.40033, Valid Loss: 3190.32580, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3216.65963, Valid Loss: 3200.23926, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3215.32177, Valid Loss: 3193.80520, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3215.23825, Valid Loss: 3197.35659, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3220.81869, Valid Loss: 3197.52757, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3216.14536, Valid Loss: 3184.32798, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3211.68744, Valid Loss: 3184.49495, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3211.72615, Valid Loss: 3183.83085, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "vade.load_model(\"model/vade2k5c2k_1.pt\")\n",
    "print(\"Initializing through GMM..\")\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.save_model(\"model/vade2k5c2k_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 8679.95242\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3233.41579, Valid Loss: 3186.49018, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3209.28563, Valid Loss: 3183.59263, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3211.07545, Valid Loss: 3183.09875, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3211.02946, Valid Loss: 3182.68724, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3211.01418, Valid Loss: 3182.51615, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3209.16660, Valid Loss: 3183.76033, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3208.18500, Valid Loss: 3181.53952, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3210.69156, Valid Loss: 3182.22657, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3216.56231, Valid Loss: 3192.80368, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3228.06418, Valid Loss: 3204.26479, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3213.43192, Valid Loss: 3181.38049, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3207.09618, Valid Loss: 3184.59422, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3207.15144, Valid Loss: 3187.67181, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3203.35760, Valid Loss: 3182.49932, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3207.64340, Valid Loss: 3183.93345, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3201.76650, Valid Loss: 3181.46259, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3201.96666, Valid Loss: 3199.70138, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3207.04547, Valid Loss: 3184.02719, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3201.83120, Valid Loss: 3181.26704, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3199.76877, Valid Loss: 3181.12559, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "vade.load_model(\"model/vade2k5c2k_2.pt\")\n",
    "print(\"Initializing through GMM..\")\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade.save_model(\"model/vade2k5c2k_3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 13186.21874\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3230.66995, Valid Loss: 3191.42460, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3198.93020, Valid Loss: 3182.39906, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3200.67083, Valid Loss: 3181.66593, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3199.40258, Valid Loss: 3183.60423, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3195.83137, Valid Loss: 3179.73581, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3197.30092, Valid Loss: 3181.60865, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3194.53059, Valid Loss: 3180.47269, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3202.62261, Valid Loss: 3182.16113, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3194.71276, Valid Loss: 3183.98300, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3197.05575, Valid Loss: 3185.75998, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3194.80184, Valid Loss: 3179.34724, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3191.17852, Valid Loss: 3181.93699, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3191.43532, Valid Loss: 3198.15559, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3195.77549, Valid Loss: 3180.40157, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3193.38911, Valid Loss: 3185.34664, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3194.80324, Valid Loss: 3184.20453, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3192.44465, Valid Loss: 3182.01015, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3190.33184, Valid Loss: 3187.10710, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3191.58419, Valid Loss: 3181.00482, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3196.09385, Valid Loss: 3179.97391, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "vade.load_model(\"model/vade2k5c2k_3.pt\")\n",
    "print(\"Initializing through GMM..\")\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I don't know the number of centroids does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing through GMM..\n",
      "#Epoch -1: Valid Loss: 3219.21289\n",
      "#Epoch   0: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: nan, Valid Loss: nan, acc: 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1496:\n",
      "Process Process-1495:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/paperspace/github/UnsupervisedDeepLearning-Pytorch/test/pacdataset.py\", line 38, in __getitem__\n",
      "    file_id = self.train0_df.iloc[idx]['file_id']\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/indexing.py\", line 1373, in __getitem__\n",
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/indexing.py\", line 1832, in _getitem_axis\n",
      "    return self._get_loc(key, axis=axis)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/indexing.py\", line 150, in _get_loc\n",
      "    return self.obj._ixs(key, axis=axis)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/frame.py\", line 2074, in _ixs\n",
      "    dtype=new_values.dtype)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/series.py\", line 264, in __init__\n",
      "    raise_cast_failure=True)\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/series.py\", line 3178, in _sanitize_array\n",
      "    subarr = np.array(data, copy=False)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7fecceac2f98>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 315, in _shutdown_workers\n",
      "    self.shutdown = True\n",
      "  File \"/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 10104) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-34cb06e28dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# vade.initialize_gmm(train_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manneal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/UnsupervisedDeepLearning-Pytorch/udlp/clustering/vade.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainloader, validloader, lr, batch_size, num_epochs, visualize, anneal)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vade.load_model(\"model/vade2k5c2k_1.pt\")\n",
    "print(\"Initializing through GMM..\")\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "# we do NOT want to initialize_gmm each time, oh wait, yes we do.\n",
    "# vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Epoch -1: Valid Loss: 3208.37063\n",
      "#Epoch   0: lr: 0.00100, Train Loss: 3107.86518, Valid Loss: 3172.76114, acc: 0.00000\n",
      "#Epoch   1: lr: 0.00100, Train Loss: 3110.91332, Valid Loss: 3176.45886, acc: 0.00000\n",
      "#Epoch   2: lr: 0.00100, Train Loss: 3110.19600, Valid Loss: 3178.79659, acc: 0.00000\n",
      "#Epoch   3: lr: 0.00100, Train Loss: 3104.94653, Valid Loss: 3183.62467, acc: 0.00000\n",
      "#Epoch   4: lr: 0.00100, Train Loss: 3118.62083, Valid Loss: 3178.66328, acc: 0.00000\n",
      "#Epoch   5: lr: 0.00100, Train Loss: 3114.10822, Valid Loss: 3199.02968, acc: 0.00000\n",
      "#Epoch   6: lr: 0.00100, Train Loss: 3120.37661, Valid Loss: 3173.95579, acc: 0.00000\n",
      "#Epoch   7: lr: 0.00100, Train Loss: 3107.35450, Valid Loss: 3185.31540, acc: 0.00000\n",
      "#Epoch   8: lr: 0.00100, Train Loss: 3095.57662, Valid Loss: 3168.91423, acc: 0.00000\n",
      "#Epoch   9: lr: 0.00100, Train Loss: 3088.22531, Valid Loss: 3190.91982, acc: 0.00000\n",
      "#Epoch  10: lr: 0.00090, Train Loss: 3084.56503, Valid Loss: 3168.53700, acc: 0.00000\n",
      "#Epoch  11: lr: 0.00090, Train Loss: 3090.34077, Valid Loss: 3179.14629, acc: 0.00000\n",
      "#Epoch  12: lr: 0.00090, Train Loss: 3080.38083, Valid Loss: 3175.84292, acc: 0.00000\n",
      "#Epoch  13: lr: 0.00090, Train Loss: 3081.48360, Valid Loss: 3183.20560, acc: 0.00000\n",
      "#Epoch  14: lr: 0.00090, Train Loss: 3086.17329, Valid Loss: 3169.25121, acc: 0.00000\n",
      "#Epoch  15: lr: 0.00090, Train Loss: 3101.88350, Valid Loss: 3204.75258, acc: 0.00000\n",
      "#Epoch  16: lr: 0.00090, Train Loss: 3145.60033, Valid Loss: 3211.37616, acc: 0.00000\n",
      "#Epoch  17: lr: 0.00090, Train Loss: 3093.38880, Valid Loss: 3174.89950, acc: 0.00000\n",
      "#Epoch  18: lr: 0.00090, Train Loss: 3086.31131, Valid Loss: 3180.41810, acc: 0.00000\n",
      "#Epoch  19: lr: 0.00090, Train Loss: 3106.79093, Valid Loss: 3176.52110, acc: 0.00000\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 8\n",
    "# vade.load_model(\"model/vade1.pt\")\n",
    "# print(\"Initializing through GMM..\")\n",
    "# vade.initialize_gmm(train_loader)\n",
    "vade.fit(train_loader, test_loader, lr=lr, batch_size=batch_size, num_epochs=epochs, anneal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
