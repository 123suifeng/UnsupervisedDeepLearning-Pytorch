{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch \"Context\" experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The general idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any module is just a semi-defined function. We should be able to insert it into any \"context\" and define it via training the entire context. \n",
    "\n",
    "More concretely, a context is a wrapper of pretrained modules on either side with a loss function that we are trying to optimize on the other end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "\n",
    "define a context object with general optimize, learning rate (scheduler), dataloader options. Basically what my `train_model` function does. But with option "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new general idea.\n",
    "\n",
    "modules are learnable functions. we just slide them in and out as needed and resuse as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import  Dataset, DataLoader\n",
    "\n",
    "\n",
    "import math\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from torchvision import datasets, transforms\n",
    "from collections import OrderedDict\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "from helperFunctions2 import get_experiment_data, extract_vector_features_from_matrix, extract_covariates_from_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nilearn import image\n",
    "from nilearn.image import mean_img\n",
    "from plot_brain import plot_brain\n",
    "import dask\n",
    "from dask.delayed import delayed\n",
    "from ipywidgets import FloatSlider, ColorPicker, VBox, jslink\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PacDataset(Dataset):\n",
    "    \"\"\"Pac  dataset. - taking one slice out of each image \"\"\"\n",
    "    @staticmethod\n",
    "    def _jload(file_id):\n",
    "        \n",
    "        full_path = PacDataset.file_template % file_id\n",
    "        img = nib.load(full_path)\n",
    "        img_data = img.get_data()\n",
    "        return img_data\n",
    "\n",
    "    def __init__(self, root_dir=\"./Pac Data/pac2018/\", train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        exp0_a = get_experiment_data(0)\n",
    "        if train:\n",
    "            train_sel = 0\n",
    "        else:\n",
    "            train_sel = 1\n",
    "        self.train0_df = pd.DataFrame(exp0_a[train_sel], \n",
    "                                columns=['file_id','cond','age','gender','vol','site'])\n",
    "        PacDataset.file_template = root_dir + \"%s.nii\"\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train0_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z_cut = 60\n",
    "        file_id = self.train0_df.iloc[idx]['file_id']\n",
    "        img_data = PacDataset._jload(file_id)\n",
    "        \n",
    "        inputs = torch.Tensor(img_data[:, :, z_cut].flatten())\n",
    "        return inputs\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "datasets = {x: PacDataset(train=(x=='train')) for x in ['val','train']}\n",
    "\n",
    "# increasing num_workers sped things up considerably.\n",
    "dataloaders = {x: torch.utils.data.DataLoader(\n",
    "    datasets[x], \n",
    "    batch_size=10, \n",
    "    num_workers=5) for x in ['train','val']}\n",
    "\n",
    "dataset_sizes = {x: len(datasets[x]) for x in ['train', 'val']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "use_gpu = use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def train_model(model, \n",
    "                criterion, \n",
    "                optimizer, \n",
    "                scheduler, \n",
    "                dataloaders,\n",
    "                num_epochs=25, \n",
    "                autoencoder=True,\n",
    "                pretrained = None):\n",
    "    \n",
    "    \"\"\" \n",
    "    general model traing function \n",
    "    could be _fit method in model class\n",
    "    \n",
    "    pretrained - is a pretrained model to transform the inputs\n",
    "    not sure if its using the gpu at this point\n",
    "    \n",
    "    uses pretained to make stacked auto-encoders\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                if autoencoder:\n",
    "                    inputs = data\n",
    "                    if use_gpu:\n",
    "                        # put inputs on gpu\n",
    "                        labels = inputs = Variable(inputs.cuda())\n",
    "                        # put model on gpu\n",
    "                        if pretrained:\n",
    "                            pretrained.cuda()\n",
    "                    else:\n",
    "                        labels = inputs = Variable(inputs)\n",
    "                    if pretrained:\n",
    "                        labels = inputs = pretrained(inputs)\n",
    "                else:\n",
    "                    inputs, labels = data\n",
    "                    # wrap them in Variable\n",
    "                    if use_gpu:\n",
    "                        inputs = Variable(inputs.cuda())\n",
    "                        labels = (labels.cuda())\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs), Variable(labels)\n",
    "                    if pretained:\n",
    "                        inputs = pretrained(inputs)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "#                 print( outputs )  #cuz ...\n",
    "#                 _, preds = torch.max(outputs.data, 1) # ????\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0] * inputs.size(0)\n",
    "                if not autoencoder:\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_module(mod, make_copy=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    mod : a nn.Module\n",
    "    make_copy: boolean to determine if a copy is returned\n",
    "    \n",
    "    Returns: a module with all parameter's gradients turned off\n",
    "    \"\"\"\n",
    "    mod_ = mod if make_copy else copy.deepcopy(mod)  \n",
    "    for p in mod_.parameters():\n",
    "        p.requires_grad = False\n",
    "    return mod_\n",
    "\n",
    "def thaw_module(mod, make_copy=False):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    mod : a nn.Module\n",
    "    make_copy: boolean to determine if a copy is returned\n",
    "    \n",
    "    Returns: a module with all parameter's gradients turned on\n",
    "    \"\"\"\n",
    "    mod_ = mod if make_copy else copy.deepcopy(mod)\n",
    "    for p in mod_.parameters():\n",
    "        p.requires_grad = True\n",
    "    return mod_\n",
    " \n",
    "def test_freeze_module():\n",
    "    m = nn.Linear(3,5)\n",
    "    mm = freeze_module(m, make_copy=True)\n",
    "    assert(isinstance(mm, nn.Module))\n",
    "    assert(m.state_dict == mm.state_dict)\n",
    "    assert(all([not p.requires_grad for p in mm.parameters()]))\n",
    "\n",
    "def test_thaw_module():\n",
    "    m = nn.Linear(3,5)\n",
    "    mm = thaw_module(m, make_copy=True)\n",
    "    assert(isinstance(mm, nn.Module))\n",
    "    assert(m.state_dict == mm.state_dict)\n",
    "    assert(all([p.requires_grad for p in mm.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_freeze_module()\n",
    "test_thaw_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ae(in_out_features, hidden_features):\n",
    "    m = nn.Sequential(nn.Linear(in_out_features, hidden_features),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_features, in_out_features))\n",
    "    return m\n",
    "\n",
    "def test_make_ae():\n",
    "    m = make_ae(10,5)\n",
    "    assert( isinstance(m, nn.Sequential))\n",
    "    assert( m[0].in_features == 10)\n",
    "    assert( m[0].out_features == 5)    \n",
    "    assert( m[2].in_features == 5)\n",
    "    assert( m[2].out_features == 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_make_ae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_ae(hidden_features, stack_in = 1, stack_ae_net = None):\n",
    "    \"\"\"\n",
    "    stack_ae_net is a sequence of two modules\n",
    "    - the first is trained sequence\n",
    "    - the second is a freshly trained ae\n",
    "    add ae_net[0],ReLU to  stack_ae\n",
    "    create new ae\n",
    "    \n",
    "    returns a nn.Sequential object of two parts, the first trained, latter to be \n",
    "    trained\n",
    "    \"\"\"\n",
    "    if stack_ae_net == None:\n",
    "        # if no stack_ae_net, make one with empty stack and new ae\n",
    "        stack_ae = None # nn.Sequential()\n",
    "        new_ae = make_ae(stack_in, hidden_features) ### left off here\n",
    "        stack_ae_net = nn.Sequential(\n",
    "            OrderedDict([('stack',stack_ae), ('ae_net',new_ae)]))\n",
    "        return stack_ae_net\n",
    "    \n",
    "    stack_ae = stack_ae_net.stack\n",
    "    if not stack_ae: stack_ae = nn.Sequential()\n",
    "    ae_net = stack_ae_net.ae_net\n",
    "    first_linear = ae_net[0]\n",
    "    old_hidden = first_linear.out_features\n",
    "    new_ae = make_ae(old_hidden, hidden_features)\n",
    "    stack_depth = len(stack_ae)\n",
    "    stack_ae.add_module(f'ae{stack_depth}', first_linear)\n",
    "    stack_ae.add_module(f'ReLU{stack_depth}', nn.ReLU())\n",
    "    stack_ae = freeze_module(stack_ae)\n",
    "    full_stack = nn.Sequential( OrderedDict(\n",
    "        [('stack', stack_ae), ('ae_net', new_ae)]))\n",
    "                              \n",
    "    return full_stack\n",
    "    \n",
    "def test_stack_ae():\n",
    "    sae = stack_ae(5, stack_in = 3) #, stack_in=5)\n",
    "    print(sae)\n",
    "    assert( isinstance(sae, nn.Sequential))\n",
    "    sae = stack_ae(8, stack_ae_net = sae)\n",
    "    sae = stack_ae(4, stack_ae_net = sae)\n",
    "    print(sae)\n",
    "    assert( isinstance(sae, nn.Sequential))\n",
    "    assert( isinstance(sae.stack, nn.Sequential))\n",
    "    assert( isinstance(sae.ae_net, nn.Sequential))    \n",
    "    assert( sae.stack[-2].out_features == sae.ae_net[0].in_features )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (stack): None\n",
      "  (ae_net): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (stack): Sequential(\n",
      "    (ae0): Linear(in_features=3, out_features=5, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (ae2): Linear(in_features=5, out_features=8, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (ae_net): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=4, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_stack_ae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (stack): None\n",
      "  (ae_net): Sequential(\n",
      "    (0): Linear(in_features=17545, out_features=2000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=2000, out_features=17545, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 0.0995 Acc: 0.0000\n",
      "val Loss: 0.0992 Acc: 0.0000\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 0.0961 Acc: 0.0000\n",
      "val Loss: 0.0973 Acc: 0.0000\n",
      "\n",
      "Training complete in 1m 40s\n",
      "Best val Acc: 0.000000\n",
      "Sequential(\n",
      "  (stack): Sequential(\n",
      "    (ae0): Linear(in_features=17545, out_features=2000, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "  )\n",
      "  (ae_net): Sequential(\n",
      "    (0): Linear(in_features=2000, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=2000, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 0.0179 Acc: 0.0000\n",
      "val Loss: 0.0181 Acc: 0.0000\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 0.0176 Acc: 0.0000\n",
      "val Loss: 0.0178 Acc: 0.0000\n",
      "\n",
      "Training complete in 0m 37s\n",
      "Best val Acc: 0.000000\n",
      "Sequential(\n",
      "  (stack): Sequential(\n",
      "    (ae0): Linear(in_features=17545, out_features=2000, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (ae2): Linear(in_features=2000, out_features=500, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "  )\n",
      "  (ae_net): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 0.0000\n",
      "val Loss: 0.0035 Acc: 0.0000\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 0.0035 Acc: 0.0000\n",
      "val Loss: 0.0035 Acc: 0.0000\n",
      "\n",
      "Training complete in 0m 36s\n",
      "Best val Acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# make initial full_stack having no stack and an ae_net (17545x500x17545)\n",
    "full_stack = stack_ae(500,121*145)\n",
    "for num_features in [500, 2000, 10]:\n",
    "    print(full_stack)\n",
    "    optimizer = torch.optim.SGD(full_stack.ae_net.parameters(), lr=0.01)\n",
    "    train_model(full_stack.ae_net, \n",
    "                    criterion, \n",
    "                    optimizer, \n",
    "                    scheduler, \n",
    "                    dataloaders,\n",
    "                    num_epochs=2, \n",
    "                    autoencoder=True,\n",
    "                    pretrained = full_stack.stack)\n",
    "    full_stack = stack_ae(num_features, stack_ae_net = full_stack )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (stack): Sequential(\n",
      "    (ae0): Linear(in_features=17545, out_features=2000, bias=True)\n",
      "    (ReLU0): ReLU()\n",
      "    (ae2): Linear(in_features=2000, out_features=500, bias=True)\n",
      "    (ReLU2): ReLU()\n",
      "    (ae4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (ReLU4): ReLU()\n",
      "  )\n",
      "  (ae_net): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=10, out_features=500, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(full_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
